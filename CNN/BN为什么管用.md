# 1. 公式：各个通道独立计算
# 2. 意义
1. 正则化
2. 加快训练收敛速度
3. 可以加大学习率
# 3. 原理：防止梯度弥散
$$
0.9^{30} = 0.04
$$

&emsp;&emsp;**BN 是一个局部化操作，它通过缩放，在每一个局部，将输入动态地规范到 $1$ 附近(神经元的敏感区域)，消除了反向长程传播的梯度弥散问题。**

&emsp;&emsp;**引入全局信息**，这个全局信息来自与当前样本以外的其他样本。当预测一个独立样本时，其他样本信息相当于正则项，使得损失函数曲面更加平滑，更容易找到最优解。
1. 正面作用：正则化
2. 负面作用：信息泄露

# 4. 好处
1. 减轻了对参数初始化的依赖，对初始化不敏感
2. 训练更快，可以提高学习率
3. 增加泛化能力，替换 `dropout`
# 5. BN 的缺陷
&emsp;&emsp;BN 依赖与 batch 的大小，当 batch 值很小时，计算的均值和方差不稳定。
这一特性，导致 BN 不适合一下场景
1. batch 非常小，比如训练资源有限无法应用较大的 batch，也比如在线学习等使用单例进行模型参数更新的场景。
2. rnn，因为它是一个动态的网络结构，同一个 batch 中训练实例长度不一，导致每一个时间步必须维持各自的统计量，使得 BN 并不能正确的使用
# 6. BN 的改进
&emsp;&emsp;添加放射变换，先使用 `BN` 训练一个相对稳定的移动平均，网络迭代后期再使用刚才的方法，称为 `Batch Renormalization`，当然 $r$ 和 $d$ 的大小必须进行限制。
# 7. BN 变种
## 7.1 BN：各个通道独立计算
## 7.2 LN：在每一层，把特征通道一起用于归一化
## 7.3 IN：把每一个特征层单独进行归一化，即限制在某一个特征通道内
## 7.4 GN：LN 和 IN 的中间体
&emsp;&emsp;`GN` 将 `channel` 方向分 `group`，然后对每一个 `group` 内做归一化，算其均值和方差。

# 8. BN 的思考
&emsp;&emsp;`BN` 机制是一个开放的问题，没有统一而确定的答案，主流观点有：
1. **分布**：[调整了数据的分布，不考虑激活函数，它让每一层的输出归一化到标准正太分布，这保证了梯度的有效性](https://arxiv.org/abs/1502.03167)
2. **学习率**：[可以使用更大的学习率，帮助跳过不好的局部极值，增加泛化能力](https://arxiv.org/abs/1806.02375)
3. **平滑**：[损失平面平滑](https://arxiv.org/abs/1805.11604)
