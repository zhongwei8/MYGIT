[详解深度学习中的梯度消失、爆炸原因及其解决方法](https://zhuanlan.zhihu.com/p/33006526)
梯度消失爆炸的解决方案主要包括：
1. 预训练加微调
2. 梯度剪切、权重正则(针对梯度爆炸)
3. 使用不同的激活函数
4. 使用 BatchNorm
5. 使用残差结构
6. 使用 LSTM 网络
# 1. 为什么使用梯度更新规则
深层网络由许多非线性网络层堆叠而来，每一个非线性层可以视为一个非线性函数 $f(x)$ (非线性来源于非线性激活函数)，整个网络可以视为一个**复合的非线性多元函数**。
$$
F(x) = f_n(f_{n - 1}(f_{n - 2} \cdots  (f_2(f_1(x))))
$$
&emsp;&emsp;我们最终的目的是希望这个多元函数可以很好的完成输入到输出之间的映射，假设不同的输入，输出的最优解是 $g(x)$，优化深度网络是为了寻找到合适的权值
$$
\begin{aligned}
Loss    &= L(g(x), F(x))   \\
        &= {\mid \mid g(x) - f(x) \mid \mid}_2^2
\end{aligned}
$$
对于非线性空间的优化问题，梯度更新可以用来寻找最优参数。
# 2. 梯度消失、爆炸
1. 梯度消失的常见场景：
    + 深层网络
    + 不合适的损失函数(如sigmoid)
2. 梯度爆炸的常见场景
    + 深层网络
    + 权值初始化太大
## 2.1 深层网络角度
&emsp;&emsp;BP 算法基于梯度下降策略，以目标的负梯度方向对参数进行调整，参数的更新过程为

$$
w \leftarrow w + \Delta w
$$
给定学习率 $\alpha$，得出 $\Delta w = -\alpha {\partial{L} \over \partial{w}}$，如果要更新第二隐藏层的权值信息，根据链式求导法则，更新梯度信息(**长程链式求导**)：
$$
\Delta w_1 = {\partial{L} \over \partial{w_2}} = {\alpha{L} \over {\partial w_4}} \cdot {\alpha{f_4} \over {\partial f_3}} \cdot {\alpha{f_3} \over {\partial f_2}} \cdot {\alpha{f_2} \over {\partial f_1}}
$$
&emsp;&emsp;总结：从深层网络角度来讲，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习情况很好，靠近输入的层学习的很慢，有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多。(**表现为基层调节失效问题**)
&emsp;&emsp;梯度消失、爆炸的根本原因在于 **反向(长程)链式传播训练法则**。


根源在 **长程链式求导** 过程
## 2.2 激活函数角度
&emsp;&emsp;计算权值更新信息的时候，需要计算前层偏导信息，如果激活函数选择不合适，比如 sigmoid，梯度消失就会更明显。
1. $\text{sigmoid}$ 表达式
$$
\sigma(x) = {1 \over {1 + e^{-x}}}  \in [0, 1]
$$
2. $\text{sigmoid}$ 的梯度
$$
\sigma^{'}(x) = \sigma(x) \cdot (1 - \sigma(x)) \in [0, 0.25]
$$
3. $\text{tanh}$ 表达式

$$
\text{tanh}(x) = {{e^x - e^{-x}} \over {e^x + e^{-x}}}  \in [-1, 1]
$$

4. $\text{tanh}$ 的梯度

$$
\text{tanh}^{'}(x) = {{4} \over {(e^x + e^{-x})^2}} \in [0, 1]
$$
# 3. 梯度消失、爆炸的解决方案
## 3.1 预训练 + 微调
&emsp;&emsp;`Hinton` 为了解决梯度问题，采用无监督逐层训练方法

1. 逐层预训练
&emsp;&emsp;每一训练一层隐节点，训练时将上一层节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入；


2. 整体微调
&emsp;&emsp;在预训练完成后，再对整个网络进行“微调”。

此思路是先寻找局部最优，然后整合起来寻找全局最优。此方法有一定的好处，但是目前应用的不是很多。

## 3.2 梯度剪切、正则
1. 背景：针对梯度爆炸
2. 梯度剪切
&emsp;&emsp;设置一个 **梯度剪切阈值**，更新梯度时，如果梯度超过这个阈值，那么就将其强制限制在这个范围内，这可以防止梯度爆炸。
3. 权重正则化
&emsp;&emsp;常见的 L1 正则化和 L2 正则化。正则化是通过网络权重做正则，限制过拟合，带正则项的损失函数形式：
$$
L = (y - W^Tx)^2 + \alpha {\mid \mid W \mid \mid}^2
$$
其中，$\alpha$ 是指正则项系数。如果发生梯度爆炸，权值的范数就会变得非常大，通过正则化项，可以限制梯度爆炸的产生。
注：在深度神经网络中，梯度消失出现的更多一些。

## 3.3 激活函数
### 3.3.1 ReLU
&emsp;&emsp;如果激活函数的导数为 1，那么就不存在梯度消失爆炸的问题，每层都可以得到相同的更新速度，ReLU 应运而生
$$
\text{Relu}(x) = \max(x, 0) = \begin{cases}
    0   \qquad x < 0    \\
    x   \qquad x > 0
\end{cases}
$$
$\text{Relu}$ 的导数在正数部分是恒等于 $1$ 的，因此深层网络中使用 $\text{Relu}$ 激活函数就不会产生梯度消失和爆炸的问题。


1. $\text{Relu}$ 的主要贡献：
    + 解决了梯度消失爆炸的问题
    + 计算方便，计算速度快
    + 加速了网络的训练
2. $\text{Relu}$ 的缺点：
    + **神经元不可逆性死掉**，负数部分恒为零。(设置小学习率可以缓解此问题)
    + **输出非零中心化**
### 3.3.2 Leak Relu
&emsp;&emsp;**Leak Relu** 解决 **Relu** 的零区间带来的问题，数学表达式为
$$
\text{LRelu}(x) = \begin{cases}
    x           \quad\qquad x > 0        \\
    k \cdot x   \qquad x \leq 0
    \end{cases}
$$
其中，$k$ 是泄露系数，一般选择 $0.1$ 或 $0.2$，也可以通过学习而来。

$\text{LRelu}$解决了零区间带来的影响，并且包含了 $\text{Relu}$ 的所有优点。

### 3.3.3 ERelu
&emsp;&emsp;$\text{ERelu}$ 激活函数也是为了解决 $\text{Relu}$ 的 **零区间** 带来的影响，数学表达式
$$
ERelu(x) = \begin{cases}
    x                   \quad x > 0 \\
    \alpha(e^x - 1)     \quad x \leq 0
\end{cases}
$$
$\text{ERelu}$ 相对于 $\text{LRelu}$ 计算更耗时一些。
## 3.4 BatchNorm
### 3.4.1 背景：解决反向传播的梯度问题
### 3.4.2 效果
1. 加速网络收敛速度
2. 提升训练稳定性
### 3.4.3 分析
&emsp;&emsp;在反向传播的过程中，经过每一层的梯度会乘以该层的权重。举例：正向传播 $f_3 = f_2(w^t \cdot x + b)$，那么反向传播中，$\partial{f_2} / \partial{x} = \partial{f_2} / \partial{f_1} \cdot w$，**反向传播式子中有 $w$ 的存在**，所以 $w$ 的大小影响了梯度的消失和爆炸，BN 就是通过对每一层的输出做 scale 和 shift 的方法，通过一定的规范化手段，把每一层神经网络任意神经元这个输入值的分布强行拉回到接近标准正太分布，即 **把非标准正太分布强制拉回标准的正太分布**，这样使得激活输入值落在非线性函数对输入比较敏感的区域 (**非线性函数的敏感区域**)，这样输入的小变化就会导致损失函数的较大变化，使得让梯度变大，避免梯度消失的问题产生。
&emsp;&emsp;梯度变大意味着学习收敛速度快，加快训练速度。
总结：
1. **反向传播式子中有 $w$ 的存在**
2. 非线性函数有一个 **敏感区域**
3. 把非标准正太分布的输入，强行规范化到标准的正太分布上去
### 3.5.4 残差结构
1. **背景：梯度消失，阻碍了网络的深度**
2. **解决方案**：跳过连接
3. 原理分析：**在原梯度中加偏置一**
$$
{\partial{L} \over \partial{x_l}} = {\partial{L} \over {x_L}} \cdot {{\partial{x_L}} \over {\partial{x_l}}} = {{\partial}{L} \over {x_L}} \cdot (1 + {\partial{} \over {\partial{x_L}} } \sum_{i = l}^{L - 1} F(x_i, W_i))
$$
&emsp;&emsp;式子中的第一个因子 $\partial{L} / {\partial}{x_L}$ 表示损失函数到达 $L$ 的梯度，**小括号中的 $1$ 表明短路机制可以无损地传播梯度**，而另外一项残差梯度则需要进过带有 `weights` 的层，梯度不是直接传递过来的。残差梯度不会那么巧全为 $-1$，而且就算其比较小，有 $1$ 的存在也不会导致梯度消失。
### 3.5.5 LSTM
&emsp;&emsp;LSTM 全称长短期记忆网络(long-short term memory networks)，是不容易发生梯度消失的，主要原因在于 LSTM 内部复杂的“门” (gates)，如下图：LSTM 通过它内部的“门”可以接下来更新的时候“记住”前几次训练的“残留记忆”，通常用于生成文本中。
