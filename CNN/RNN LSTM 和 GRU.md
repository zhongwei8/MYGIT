# 4 LSTM 和 GRU 的异同
1. 对 memory 的控制
+ LSTM：用输出门，传输给下一个单元
+ GRU：直接传递给下一个单元
2. 输入门和重置门的作用位置不同
+ LSTM：计算新记忆时，不对上一时刻的信息作任何控制，而是用遗忘门独立地实现这一点
+ GRU：计算新记忆时，利用重置门对上一时刻的信息进行控制
2. input gate 和 reset gate 作用位置不同
3. 相似
&emsp;&emsp;从 $t - 1$ 到 $t$ 的更新，都引入了**加法**：能够**防止梯度弥散**，因此 LSTM 和 GRU 都比一般的 RNN 效果更好。
