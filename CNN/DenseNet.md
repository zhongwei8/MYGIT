# 1. 背景

**网络存在较大冗余性**

&emsp;&emsp;通过探索 dropout 值的比例对性能的影响可以估算这个冗余。既然丢掉某些层间连接或者整个层不影响性能，就说明这一层学习到的非线性转变很小，既然转变很小，那么每一层学习几百个通道，有必要吗？**这几百个通道，正是万恶的计算量所在。**
# 2. 主要措施
&emsp;&emsp;考虑到网络的冗余性，densenet 同时做了两件事
1. **直连**：网络中的每一层都直接与前面层连接，提高特征利用率；
2. **窄化**：网络的每一层都设计地很窄，也就是卷积的输出通道数通常很小，只有几十，该层学习非常少的特征图并与输入 concat 连接。
DenseNet 借鉴了 ResNet，是 ResNet 的升级版本。ResNet 的每一个 **Block** 会有一个 skip connect，而 DenseNet 会在每层 **conv** 间有一个 skip connect
# 3. 效果
&emsp;&emsp;实现了资源的最大化利用和计算量的压缩。ImageNet 分类数据集上达到了同样的准确率，DenseNet 所需的**参数量和计算量不到 ResNet 的一半**。



# 附：
缓解过拟合办法：BN、Dropout、DA
